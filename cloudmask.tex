\FILE{Cloudmask.tex}

\subsection{MLCommons Cloudmask Workflow}
\label{cloudmask-workflow}

Cloudmask is a program that develops a model to classify sections of
satellite images as either containing clouds or clear sky by using
machine learning. This is beneficial for temperature measurement and
meteorology.  Information regarding Cloudmask can be found on its
GitHub page~\cite{www-cloudmask}.
One of our goals is to run Cloudmask for benchmarking.
As benchmarking Cloudmask requires
several phases and scripts, including a mixture of shell scripts and
Python scripts, leveraging Cloudmesh-cc provides a much easier runtime
instead of manually issuing many commands at a terminal.
We have created a sample workflow for an HPC
computer using UVA's Rivanna HPC. 
This workflow can easily be adapted to other remote machines. In this
particular workflow, we execute the benchmarks on a number of different
CUDA cards (see Figure~\ref{fig:cloudmaskwf}. The code internally utilizes
our cloudmesh-vpn component that provides the ability to
connect to the UVA VPN from Python, then fetches the data and executes
the various benchmarks once the data is available.


\begin{figure}[htb]
  \resizebox{1.0\columnwidth}{!}{
   \digraph{cloudmaskwf}{
     rankdir=TB;
     node [shape=box style=rounded];
      start -> "connect vpn" -> "fetch data" -> "v100 benchmark" -> "disconnect vpn" -> end;
      "fetch data" -> "p100 benchmark" ->  "disconnect vpn";
      "fetch data" -> "a100 benchmark" ->  "disconnect vpn";
      "fetch data" -> "k80 benchmark" ->  "disconnect vpn";
      "fetch data" -> "rtx2080 benchmark" ->  "disconnect vpn";    
    }
  }
  \vspace{-1cm}
  \caption{Workflow for Cloudmask}\label{fig:cloudmaskwf}
\end{figure}


The workflow will take approximately 24 hours to run if resources are
available. The workflow iterates through the five GPUs available on
Rivanna, including A100, V100, P100, RTX2080, and K80, and runs the program
three times on each GPU. Each run trains the model with 10, 30, and 50
epochs for benchmarking.  Upon completing a run, the logs and
benchmarks are written into a results folder.

