\FILE{quickstart.tex}

\section{Cloudmesh-cc Interfaces}

In this section we explain the various ways of interfacing with cloudmesh-cc.

It is important to note that we have a {\em commandline mode} that interfaces directly with the backends while not requiering a service. This includes an easy to use python API.

In addition we have used this API to implement a {\em service mode}, so we an stand up a REST service as well as a GUI that can ba accessed through a Web browser.
Please note that the service mode can also be access through te command line in a terminal. To distinguish how we operate cloudmesh-cc we use the term {\em mode} to deliniate it from a command that is entered ina terminal to query the status of the workflow.

We will now discuss tes interfaces in more detail and also showcase
hwo we can access them.


\subsection{Python API}

Cloudmesh-cc is implemented in Python. Extensive documentation is
available in GitHub and through GitHub action it is automatically
updated \cite{github-cloudmesh-cc}.  We distingush two main classes
that are easy to use. The first is a {\em Job class} that can adapted
to include new computational resource types for executing jobs. The
second is a {\em Workflow class} to ccordinate the execution of
multiple jobs. Selected methods of the Job class include are listed in
Figure~\ref{fig:code-job}. Importat to note is that the code for the
scripts are all managed locally and a syncronization step is invoked
prior to one running the job as to assuer that the lates script and
code to be executed on the compute resource is available.

\begin{figure}[htb]
\caption{Pseudo code for the Job class with selected methhods}
\label{fig:code-job}
\begin{minted}[breaklines]{bash}
class Job:
   ...
   def clear():
      """Clear job progress."""
   def create(filename=None, script=None, exec=None):
      """Create a template for the script with progress."""
   def get_log(refresh=True):
      """Get the log of the job."""
   def get_pid(refresh=False):
      """Get the pid that the job is running within determined by the compute resource it is running on."""
   def get_progress(refresh=False):
       """Get the progress of the job from the compute resource."""
   def get_status(refresh=False):
       """Get the status of the job."""
   def kill():
       """Kill the job."""
   def run():
       """Run the job."""
   def sync():
       """Synchronise the current directory with the remote. Copies the shell script to the experiment directory and ensures that the file is copied with the sync command
   def watch(period=10):
       """Watch the job and check for changes in the given period."""

\end{minted}
\end{figure}

\begin{figure}[htb]
\caption{Pseudo code for the Job class with selected methhods}
\label{fig:code-workflow}
\begin{minted}[breaklines]{bash}
class Workflow(...):
    def add_dependencies(dependency)
        """Add a job dependency to the workflow (and the graph)."""
    add_dependency(source, destination)
        """Add a job dependency to the workflow (and the graph)."""
    def add_job( ... ):
        """Add a job to the workflow with appropriate parameters."""
    def display(filename=None, name='workflow', first=True)
        """Show the graph of the workflow."""
    def job(name):
        """Return the details of a job within the workflow."""
    def load(filename, clear=True)
        """Load the workflow."""
    def remove_job(name, state=False)
        """Remove a particular job from the workflow."""
    def remove_workflow()
        """Delete workflow from the local file system."""
    def run_parallel( ... )
        """Run a workflow in a parallel fashion."""
    def run_topo(order=None, dryrun=False, show=True, filename=None)
        """Run the workflow in a topological order."""
    def save(filename=None):
        """Save the workflow."""
    def save_with_state(filename, stdout=False):
        """Save the workflow with state."""
    def sequential_order():
        """Return a list of the topological order of the workflow."""
    property table
        """Return a table of the workflow."""
    def update_progress(name):
        """Manually update the progress of a job according to its log file."""
    def update_status(name, status):
        """Manually update a jobâ€™s status."""
\end{minted}
\end{figure}


\begin{figure}[htb]
\caption{Pseudo code for the Job class with selected methods}
\label{fig:code-workflow-rest}
\begin{minted}[breaklines]{bash}
class RESTWorkflow
    def add_job(workflow_name, **kwargs):
        """Add a job to the workflow."""
    def delete_workflow(workflow_name, job_name=None):
        """Delete a workflow by using REST."""
    def get_workflow(workflow_name, job_name=None):
        """Retrieve a workflow by using REST."""
    def list_workflows():
        """Return a list of workflows that is found within the server."""
    def run_workflow(workflow_name, run_type='topo', show=True):
        """Run a workflow by using REST."""
    def upload_workflow(directory=None, archive=None, yaml=None):
        """Upload a workflow by using REST."""
\end{minted}
\end{figure}

\begin{figure}[htb]
\caption{Pseudo code for the Job class with selected methods}
\label{fig:code-workflow-example}
\begin{minted}[breaklines]{bash}
    w = Workflow(filename="workflow.yaml")
    w.load(filename="abc.yaml") <- loads in the graph, but will save it still to workflow.yaml
    w.add(filename="add.yaml") <- adds a new workflow into the existing one.
    w.add_job(name="a",
                command="hostname",
                user=None, # bug if kind is local and user is None, we do not need user, but can take local user
                host=None, # bug:  if host is none and kind is local use localhost
                label="a",
                kind="local",
                status="ready",
                progress=0)
    w.add_job(name="b", command="ls")
    w.add_job(name="c", command="pwd")
    w.add_job(name="d", command="uname -a")
    w.add_dependencies(dependency="a,b,d")
    w.add_dependencies(dependency="a,c,d")
    w.run()
\end{minted}
\end{figure}

\subsection{Commandline Mode}

The commandline mode is an implementation that does not use a
backend service. It runs in the terminal until the workflow is
completed. All states of the jobs are managed on the compute service on
which the job is run and the status is replicated into the client on
demand. This allows easy reporting of the status through a table and a
graph display using client based rendering tools. 


The Cloudmesh Compute Cluster service allows you to execute analytical,
computational workflows which run programs on remote compute resources.
We specify not only a list of tasks to be executed, but the tasks can
also have dependencies between each other expressed through a direct,
cyclic graph. Currently, we have predefined task types that can execute
shell scripts, Python programs, and Python notebooks, all of which can
be executed on local or remote compute resources.


We can specify such workflows in a YAML file making it very easy to
create and manage them. To get you started, we have developed a
quickstart example that you can enhance to get acquainted with the
system.

Once a workflow is specified, it can be executed a using one of our
interfaces. This includes a Python API and a command line interface
without a REST service. In addition we have, with the help of the Python
API, developed a REST service so that integration into other frameworks
can be facilitated through REST calls. The availability of the REST
service is important to access it from non Python frameworks, but also
allows the easy interaction through a Web-based Portal.

Let us demonstrate a number of selected features with the help of this
quickstart. We will only focus on using the REST service.

\subsection{Setup}\label{setup}

To start, the code is needed and we assume you are standing in the
cloudmesh-cc directory. Use the following commands

\smallskip
\begin{minted}[breaklines]{bash}
$ git clone https://github.com/cloudmesh/cloudmesh-cc.git
$ cd cloudmesh-cc
$ pip install -e .
\end{minted}
\smallskip

We also assume you start the service with

\smallskip
\begin{minted}[breaklines]{bash}
$ cms cc start --reload
\end{minted}
\smallskip

\subsection{Creating a simple example}\label{creating-a-simple-example}

First let us create a simple workflow example that we already have
included for you in our source code. We will use this workflow and copy
it into a temporary directory:

\smallskip
\begin{minted}[breaklines]{bash}
$ mkdir -p /tmp/workflow-example
$ cp tests/workflow-example/workflow-example.yaml /tmp\
/workflow-example
$ cp tests/workflow-example/*.sh /tmp/workflow-example
\end{minted}
\smallskip

\TODO{create refs, only one for the dir and than we can refer to the others as is, we nit github ref}

We like you to inspect the example so you get an overview of how to
define a workflow that is located in
\href{https://github.com/cloudmesh/cloudmesh-cc/tree/main/tests/workflow-example}{GitHub} and is located in the folder \emph{tests/workflow-example/}

\begin{itemize}
\item
  \href{https://github.com/cloudmesh/cloudmesh-cc/blob/main/tests/workflow-example/workflow-example.yaml}{workflow-example.yaml}
\item
  \href{https://github.com/cloudmesh/cloudmesh-cc/blob/main/tests/workflow-example/fetch-data.sh}{fetch-data.sh}
\item
  \href{https://github.com/cloudmesh/cloudmesh-cc/blob/main/tests/workflow-example/compute.sh}{compute.sh}
\item
  \href{https://github.com/cloudmesh/cloudmesh-cc/blob/main/tests/workflow-example/analyze.sh}{analyze.sh}
\end{itemize}

Now we can test a selected number of ways on how to interact with the
service. We showcase here how to

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\Alph{enumi}.}
  \item
    upload workflows as tar/xz file of a self containing directory, or
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\Alph{enumi}.}
  \setcounter{enumi}{1}
  \item
    upload all files in a directory recursively
  \end{enumerate}
\end{itemize}

\subsection{A. Upload and run a workflow embedded in an archive
file}\label{a.-upload-and-run-a-workflow-embedded-in-an-archive-file}

We can upload an archive file which contains all the workflow files,
including the yaml specification file and the scripts. Providing an
archive file such as a \mintinline{bash}|.tar|, \mintinline{bash}|.tar.gz|, or \mintinline{bash}|.xz|.
may enable the workflow to be better suited towards portability and
simplicity, where one can share an archive file with another user. We
will choose here a tar file as example.

\subsubsection{Create tar archive file}\label{create-tar-archive-file}

Before uploading a \texttt{tar} archive file, we must first create it
using the previous example yaml and scripts that we copied.

\smallskip
\begin{minted}[breaklines]{bash}
$ tar -C /tmp/workflow-example -cf workflow-example.tar .
\end{minted}
\smallskip

\subsubsection{\texorpdfstring{Option 1: Upload via
\texttt{curl}}{Option 1: Upload via curl}}\label{option-1-upload-via-curl}

We can upload the archive file by using the \texttt{curl} terminal
command as follows:

\smallskip
\begin{minted}[breaklines]{bash}
$ curl -X 'POST' 'http://127.0.0.1:8000/ workflow?archive=workflow-example.tar'     -H 'accept: application/json' -d ''
\end{minted}
\smallskip

\subsubsection{\texorpdfstring{Option 2: Upload via
\texttt{/docs}}{Option 2: Upload via /docs}}\label{option-2-upload-via-docs}

As the service is using also an OpenAPI 2.0 specification the workflow
can also be uploaded implicitly through the specification GUI. Navigate
to \texttt{http://127.0.0.1:8000/docs} and use the POST Upload method.

\begin{figure}
\centering
\includegraphics[width=0.7\columnwidth]{images/upload_api.png}
\caption{Browser API GUI for Cloudmesh Compute Cluster}
\end{figure}

Please click \texttt{Try\ it\ out} and then enter \newline
\mintinline{bash}|~/cm/cloudmesh-cc/workflow-example.tar| in the
\mintinline{bash}|archive| field and then click Execute.

\subsubsection{Option 3: Upload via the Python
API}\label{option-3-upload-via-the-python-api}

As we use a REST service, we can also easily upload the workflow through
a Python enabled REST call. We will use Python requests to demonstrate
this upload feature.

\smallskip
\begin{minted}[breaklines]{python}
import requests

r = requests.post('http://127.0.0.1:8000/ workflow?archive=workflow-example.tar')
print(r)
print(r.text)
\end{minted}
\smallskip

Printing \texttt{r} returns the response code from the API (a code of
200 indicates success). Printing \texttt{r.text} returns the message
from the API, such as a success or error message.

\subsubsection{Option 4: Upload with a third party REST
framework}\label{option-4-upload-with-a-third-party-rest-framework}

Naturally if you like to use a different REST API you can do so. You can
also use different programming languages and we leave it up to you to
choose the framework of your choice to interact with the REST service,
popular choices are JavaScript, Go, C/C++, matlab, and R, to name a few.

\subsection{B. Upload a dir that contains workflow
directory}\label{b.-upload-a-dir-that-contains-workflow-directory}

To increase flexibility and allow quick experiments, users can specify a
workflow directory which contains the yaml specification file and the
scripts. This way, pre-archival of the directory is not needed. The
program sets up the workflow by copying the necessary files from the
specified directory.

There are three different ways to upload the dir: via \texttt{curl} on
the command line, via the browser GUI, and via the Python API.

\subsubsection{\texorpdfstring{Option 1: Upload via
\texttt{curl}}{Option 1: Upload via curl}}\label{option-1-upload-via-curl-1}

A workflow can be uploaded easily with a curl command from the command
line. On the command line execute:

\smallskip
\begin{minted}[breaklines]{bash}
$ curl -X 'POST' 
  'http://127.0.0.1:8000/workflow?directory= ~/cm/cloudmesh-cc/tests/workflow-example' 
  -H 'accept: application/json' 
  -d ''
\end{minted}
\smallskip

\subsubsection{\texorpdfstring{Option 2: Upload via
\texttt{/docs}}{Option 2: Upload via /docs}}\label{option-2-upload-via-docs-1}

Also here one can upload the needed files with the OpenAPI specification
interface on the service. Navigate to
\mintinline{bash}|http://127.0.0.1:8000/docs| and use the POST Upload method. 
Click \mintinline{bash}|Try it out| and then enter

\begin{minted}[breaklines]{bash}
/cm/cloudmesh-cc/tests/workflow-example
\end{minted}

in the
directory field and then click Execute.

\subsubsection{Option 3: Upload via the Python
API}\label{option-3-upload-via-the-python-api-1}

Accessing the upload from the Python API is very easy. We will use
python requests to demonstrate the upload of the workflow.

\smallskip
\begin{minted}[breaklines]{python}
import requests
r = requests.post('http://127.0.0.1:8000 /workflow?directory= ~/cm/cloudmesh-cc/tests/workflow-example')
print(r)
print(r.text)
\end{minted}
\smallskip

Printing \mintinline{bash}|r| returns the response code from the API (a code of
200 indicates success). Printing \mintinline{bash}|r.text| returns the message
from the API, such as a success or error message.

\subsection{Parameters to the Upload a workflow with a REST
call}\label{parameters-to-the-upload-a-workflow-with-a-rest-call}

The upload REST URL can take different parameters, such as

\begin{minted}[breaklines]{bash}
directory=name
archive=name.tar.gz
archive=name.tgz
archive=name.xz
yaml=name
\end{minted}

The semantic of the upload is specified through its parameter. Only one
of them can be used at a time.

The directory parameter indicates that the contents of a specified
directory will be transferred into a workflow.

The archive parameter indicates that an archive file, such as a
\texttt{tar} file, will be extracted and its contents will be
transferred into a workflow. Please note that the \texttt{archive} and
\texttt{directory} parameters can not be used in the same REST call.

The yaml parameter indicates that only a yaml file will be uploaded
without any corresponding scripts. Uploading a yaml file by itself
allows for a script specified by the yaml to be uploaded later. The yaml
can even work without scripts by using the \texttt{exec} specification
within the yaml.

\subsection{Run the workflow}\label{run-the-workflow}

To run, navigate to homepage at \texttt{http://127.0.0.1:8000/} and
click the workflow-example on the left side. Then click Run.
